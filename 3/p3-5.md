1简介

数据质量评价，审核数据质量

清理数据的方法

根据具体情况变化，但是仍然有具体的项目获得

2什么是数据清理

迭代的过程

可能出现数据缺失，混淆

更改格式

3问题部分

不好看的都是那种

4来源

往往因为各种原因

5测试数据质量

五种类型

有效性：是否符合我们需要的数据模式

精度：符合现实么，需要一些黄金标准（实际上需要）

完整度：全么

一致性：跟其他数据格式一致

统一性：单位一致么

6度量质量的难点

7针对清理的蓝图

1.审核数据

2.确定数据清理计划：

明确原因，定义问题，测试

3执行计划

4实在是不行还得手动整理数据

8使用蓝图的示例

芝加哥地图的使用

利用Python和正则表达式，每个都因此返回街道名

由此得到完全的理解

主要函数是审核，用来对XML格式进行理解

但是此处实现是用C语言

得到一个有所依据的输出

9审查有效性

用有效性指标来观察

我们肯呢过有必填的，也有唯一的，或者是必须有的一个键值对

也可能有交叉约束互相影响的

某个字典可以是数组等等

10维基百科信息数据集

信息框数据，对文本数据的一种补充

存储在Mongodb里面是最终目的

成为CSV之后还得往里面找如何放进去数据

11审查字段的约束条件

审查可能需要理解的数据

查到有些不太合适的数据从单位上还是数据间的联系上发现不合理的地方

12修正有效性

```python
"""
Your task is to check the "productionStartYear" of the DBPedia autos datafile for valid values.
The following things should be done:
- check if the field "productionStartYear" contains a year
- check if the year is in range 1886-2014
- convert the value of the field to be just a year (not full datetime)
- the rest of the fields and values should stay the same
- if the value of the field is a valid year in the range as described above,
  write that line to the output_good file
- if the value of the field is not a valid year as described above, 
  write that line to the output_bad file
- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org
- you should use the provided way of reading and writing data (DictReader and DictWriter)
  They will take care of dealing with the header.

You can write helper functions for checking the data and writing the files, but we will call only the 
'process_file' with 3 arguments (inputfile, output_good, output_bad).
"""
import csv
import pprint

INPUT_FILE = 'autos.csv'
OUTPUT_GOOD = 'autos-valid.csv'
OUTPUT_BAD = 'FIXME-autos.csv'

def process_file(input_file, output_good, output_bad):
    # store data into lists for output
    data_good = []
    data_bad = []
    with open(input_file, "r") as f:
        reader = csv.DictReader(f)
        header = reader.fieldnames
        for row in reader:
            # validate URI value
            if row['URI'].find("dbpedia.org") < 0:
                continue

            ps_year = row['productionStartYear'][:4]
            try: # use try/except to filter valid items
                ps_year = int(ps_year)
                row['productionStartYear'] = ps_year
                if (ps_year >= 1886) and (ps_year <= 2014):
                    data_good.append(row)
                else:
                    data_bad.append(row)
            except ValueError: # non-numeric strings caught by exception
                if ps_year == 'NULL':
                    data_bad.append(row)

    # Write processed data to output files
    with open(output_good, "w") as good:
        writer = csv.DictWriter(good, delimiter=",", fieldnames= header)
        writer.writeheader()
        for row in data_good:
            writer.writerow(row)

    with open(output_bad, "w") as bad:
        writer = csv.DictWriter(bad, delimiter=",", fieldnames= header)
        writer.writeheader()
        for row in data_bad:
            writer.writerow(row)

def test():

    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)


if __name__ == "__main__":
    test()
```

13审查准确率

下载下来的数据其实仍并不准确，为了准确审核

用来确认的方法就是跟ISO标准确认

14审查准确率2

用ETL软件可以解决很多问题

用代码解决问题的基础知识

用PyMongoDB发现：

数组数据，列转移，正则表达式

15审查完整性

处理的问题必然是一致的，比如缺失值，缺失整条

根据已有数据处理先推断是否有足够的记录

16审查一致性

输入项也未必一样，填写了两个不同的地址

来源多样的话，同样的问题要找出最可信的问题

17一致性

一个时间内必须得是一类数据

18审查均匀性

使用相同的测度值

用一个多段的if判断，是否格式合适

此外再检查是否在一定范围内

19更多信息修正

印刷错误

根据已有信息验证（交叉验证）

数据缩写更新

编码标准改变

数据增强混合

20结论

审核与清理数据



